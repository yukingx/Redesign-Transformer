# Redesign of Efficient Transformers

### **PapersğŸ“„ **  
I am reading these papers:  
âœ… [Simplifying Transformer Blocks](https://arxiv.org/pdf/2311.01906.pdf)  
â˜‘ï¸ [Blockwise-Parallel-Transformer-for-Long-Context-Large-Models](https://arxiv.org/pdf/2305.19370.pdf)  

### **Goals ğŸš€**
âœ… Understand the concept of Attention, Self-Attention, and Multi-Head Attention.
â˜‘ï¸ Understand the concpet of Key, Query, and Value.
â˜‘ï¸ Understand the concpet of Positional Encoding.
â˜‘ï¸ Understand the concpet of Encoder and Decoder 
â˜‘ï¸ Test and Understand a training process of the original transformer at the code level.
â˜‘ï¸ Design and Implement a parallelized transformer block of the encoder
â˜‘ï¸ Design and Implement a simplified transformer block of the encoder

### **Related GitHub Works:**
ğŸŒ [pytorch-llama](https://github.com/hkproj/pytorch-llama/tree/main) - PyTorch implementation of LLaMA by Umar Jamil.  

### **Articles:**
â˜‘ï¸ [Beyond Token Prediction: the post-Pretraining journey of modern LLMs](https://amatriain.net/blog/postpretraining)  
â˜‘ï¸ Add more articles. 
