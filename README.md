# Redesign of Efficient Transformers

[![Hits](https://hits.sh/github.com/silentsoft/hits.svg)](https://hits.sh/github.com/silentsoft/hits/)

### ** Transformer **
<!--
![](original_transformer.png)
-->
<img src="original_transformer.png" height="600">

### ** PapersğŸ“„ **  

Hand Crafted Transformers:  
â˜‘ï¸ [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)  
â˜‘ï¸ [ViT]()  
â˜‘ï¸ [Mobile ViT]()  
â˜‘ï¸ [Swin-T]()  
â˜‘ï¸ [TinyViT]()  
â˜‘ï¸ [MinViT]()  
â˜‘ï¸ [EfficientViT]()  

Block-level Efficient Transformers:  
âœ… [Simplifying Transformer Blocks](https://arxiv.org/pdf/2311.01906.pdf)  
â˜‘ï¸ [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/pdf/2103.16302.pdf)  
â˜‘ï¸ [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit](https://arxiv.org/pdf/2306.17759.pdf)  
â˜‘ï¸ [Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention](https://arxiv.org/pdf/2112.14000.pdf)  
â˜‘ï¸ [Training-free Transformer Architecture Search](https://arxiv.org/pdf/2203.12217.pdf)    
â˜‘ï¸ [Blockwise-Parallel-Transformer-for-Long-Context-Large-Models](https://arxiv.org/pdf/2305.19370.pdf)  
â˜‘ï¸ [BI-DIRECTIONAL BLOCK SELF-ATTENTION FOR FAST AND MEMORY-EFFICIENT SEQUENCE MODELING](https://arxiv.org/pdf/1804.00857.pdf)  

NAS for Efficient Transformers:  
â˜‘ï¸ [The Evolved Transformer](https://arxiv.org/pdf/1901.11117.pdf)  
â˜‘ï¸ [AutoFormer: Searching Transformers for Visual Recognition](https://arxiv.org/pdf/2107.00651.pdf)  
â˜‘ï¸ [COSFORMER : RETHINKING SOFTMAX IN ATTENTION](https://arxiv.org/pdf/2202.08791.pdf)  
â˜‘ï¸ [NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning](https://arxiv.org/pdf/2306.10792.pdf)  

Trainging-Free NAS
[]()

### ** Goals ğŸš€ **
âœ… Understand the concept of Attention, Self-Attention, and Multi-Head Attention.  
â˜‘ï¸ Understand the concpet of Key, Query, and Value.  
â˜‘ï¸ Understand the concpet of Positional Encoding.  
â˜‘ï¸ Understand the concpet of Encoder and Decoder.  
â˜‘ï¸ Test and Understand a training process of the original transformer at the code level.  
â˜‘ï¸ Design and Implement a parallelized transformer block of the encoder.  
â˜‘ï¸ Design and Implement a simplified transformer block of the encoder.  

### ** Related GitHub Works: **
ğŸŒ [MS Cream](https://github.com/microsoft/Cream/tree/main) - A Collection of NAS and Vision Transformer Work: ('20) Cream, ('21) AutoFormer, AutoFormerV2, ('22) TinyViT, MinViT, CDARTS, and ('23) TinyCLIP, EfficientViT 
ğŸŒ [NAS-With-Code](https://github.com/xiaoiker/NAS-With-Code)
ğŸŒ [Neural-Architecture-Search-PaperAndCode-Sunmary](https://github.com/LiuTingWed/Neural-Architecture-Search-PaperAndCode-Sunmary)
ğŸŒ [Neural Tangent Kernel Papers](https://github.com/kwignb/NeuralTangentKernel-Papers)
â˜‘ï¸ Add more repositories. 

### ** Articles: **
â˜‘ï¸ [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) - Code Based Explanation (So Good for Newbies!)  
â˜‘ï¸ Vision Transformer (ViT)  
â˜‘ï¸ GPT 2  
â˜‘ï¸ [Transformer Details Not Described in The Paper](https://tunz.kr/post/4)  
â˜‘ï¸ [Transformer, GPT-3,GPT-J, T5 and BERT.](https://aliissa99.medium.com/transformer-gpt-3-gpt-j-t5-and-bert-4cf8915dd86f)  
â˜‘ï¸ [Beyond Token Prediction: the post-Pretraining journey of modern LLMs](https://amatriain.net/blog/postpretraining)  
â˜‘ï¸ [Understanding the Neural Tangent Kernel](https://rajatvd.github.io/NTK/)
â˜‘ï¸ Add more articles.  

Korean:  
â˜‘ï¸ [ì…€í”„ ì–´í…ì…˜ ë™ì‘ ì›ë¦¬](https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/)
<!-- 
- https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice?tab=readme-ov-file 
-->

### ** Videos: **
Korean:  
â˜‘ï¸ [[ë”¥ëŸ¬ë‹ ê¸°ê³„ ë²ˆì—­] Transformer: Attention Is All You Need (ê¼¼ê¼¼í•œ ë”¥ëŸ¬ë‹ ë…¼ë¬¸ ë¦¬ë·°ì™€ ì½”ë“œ ì‹¤ìŠµ)](https://www.youtube.com/watch?v=AA621UofTUA)

### ** Free Books: **
â˜‘ï¸ [ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/book/2155) - Transformer Fundamentals for NLP (Korean Only)
<!--
- (TensorFlow) https://github.com/ukairia777/tensorflow-nlp-tutorial
- (PyTorch) https://wikidocs.net/book/2788
- (Slides) https://www.slideshare.net/wonjoonyoo/ss-188835227
-->
â˜‘ï¸ [PyTorchë¡œ ì‹œì‘í•˜ëŠ” ë”¥ ëŸ¬ë‹ ì…ë¬¸](https://wikidocs.net/book/2788) - See the parts of NLP (Korean Only)

### ** Implementation **
I plan to apply these tools for my implementation:  
âœ… PyTorch, Conda, GPU Server with A5000 x 8, tmux for session management  
â˜‘ï¸ [WandB ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì˜ í•™ìŠµì„ ì¶”ì í•˜ëŠ” ë°©ë²•](https://teddylee777.github.io/machine-learning/wandb/)  
