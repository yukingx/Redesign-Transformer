# Redesign of Efficient Transformers
#### Contributor(s): yukingx@gmail.com (I am interested in variants of the Vision Transformer)

[![Hits](https://hits.sh/github.com/yukingx/Redesign-Transformer.svg)](https://hits.sh/github.com/yukingx/Redesign-Transformer/)

### ** Transformer **
<!--
![](original_transformer.png)
-->
<img src="original_transformer.png" height="600">

### ** Goals ğŸš€ **
âœ… Understand the concept of Attention, Self-Attention, and Multi-Head Attention.  
â˜‘ï¸ Understand the concpet of Key, Query, and Value.  
â˜‘ï¸ Understand the concpet of Positional Encoding.  
â˜‘ï¸ Understand the concpet of Encoder and Decoder.  
â˜‘ï¸ Test and Understand a training process of the original transformer at the code level.  
â˜‘ï¸ Design and Implement a parallelized transformer block of the encoder.  
â˜‘ï¸ Design and Implement a simplified transformer block of the encoder.  
<!--
â˜‘ï¸ Design and Implement a PA-Former, Parallel Transformer with Adaptively Shaped Attention  
â˜‘ï¸ Write and Submit PA-Former to AI Conferences such as ICPR 2024, ACCV 2024, etc.  
-->

### ** PapersğŸ“„ **  

### *** GPT Implementation: **
â˜‘ï¸ [It's GPT Time!](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-1-865bec6fbcce)  
â˜‘ï¸ [Optimize!](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-2-f8653926272f)  
â˜‘ï¸ [Attention!](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-3-a42313db1421)  
â˜‘ï¸ [Attention is all you need!](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-4-319365968713)  
â˜‘ï¸ [Transformer Networks](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-5-d5c0cbfec7de)  
â˜‘ï¸ [Karpathy's Original Video: Letâ€™s Build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=20s)  

### ** Related GitHub Works: **
ğŸŒ [Transformers](https://github.com/huggingface/transformers) - A Collection of State-of-Arts Transformers with HuggingFace Code Links
â˜‘ï¸ Add more repositories.  

### ** Articles: **
â˜‘ï¸ [[AI] A Comprehensive Review of Transformers: from BERT to ChatGPT](https://medium.com/@vlad_zh/a-comprehensive-review-of-transformers-from-bert-to-chatgpt-f7dfe2b23043)  
â˜‘ï¸ [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) - Code Based Explanation (So Good for Newbies!)  
â˜‘ï¸ Vision Transformer (ViT)  
â˜‘ï¸ GPT 2: [It's GPT Time!](https://lnkd.in/gAAiWe3q) => [Optimize!](https://lnkd.in/gYVUq7e7) => [Attention!](https://lnkd.in/gFqEyiC8) => [Attention is all you need!](https://lnkd.in/gsRAH_cY) => [Transformer Networks](https://lnkd.in/gBmNKyrz)  
â˜‘ï¸ [Transformer Details Not Described in The Paper](https://tunz.kr/post/4)    
â˜‘ï¸ [Transformer, GPT-3,GPT-J, T5 and BERT.](https://aliissa99.medium.com/transformer-gpt-3-gpt-j-t5-and-bert-4cf8915dd86f)    
â˜‘ï¸ [Beyond Token Prediction: the post-Pretraining journey of modern LLMs](https://amatriain.net/blog/postpretraining)    
â˜‘ï¸ [Understanding the Neural Tangent Kernel](https://rajatvd.github.io/NTK/)  
â˜‘ï¸ [Mixture-of-Experts (MoE): The Birth and Rise of Conditional Computation](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth) 
â˜‘ï¸ Add more articles.  
<!-- 
Korean::  
â˜‘ï¸ [Large Language Model (1) : Foundation Model](https://jins-sw.tistory.com/48)  
â˜‘ï¸ [Large Language Model (2) : LLMì„ ê°€ëŠ¥ì¼€í•œ ì‚¼ë°•ì](https://jins-sw.tistory.com/49)  
â˜‘ï¸ [Large Language Model (3) : In-Context Learning, ë‚¨ì€ ì´ì•¼ê¸°ë“¤](https://jins-sw.tistory.com/51)  
â˜‘ï¸ [BERT ë…¼ë¬¸ì •ë¦¬](https://tmaxai.github.io/post/BERT/)  
â˜‘ï¸ Add more articles.  
â˜‘ï¸ [ì…€í”„ ì–´í…ì…˜ ë™ì‘ ì›ë¦¬](https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/)
- https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice?tab=readme-ov-file 
âœ… [ìƒê°ì˜ ì‚¬ìŠ¬(Chain-of-Thought) í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ì‘ì—…ì˜ ì„±ëŠ¥ ë†’ì´ê¸°](https://www.ncloud-forums.com/topic/63/)
-->

### ** Videos: **
â˜‘ï¸ [Karpathy's Video: Letâ€™s Build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=20s)  
â˜‘ï¸ [New KOALA LLM - Ignite Your Professional Career in AI](https://www.youtube.com/watch?v=ePoCYL_5rDM)
<!-- 
Korean:  
â˜‘ï¸ [[ë”¥ëŸ¬ë‹ ê¸°ê³„ ë²ˆì—­] Transformer: Attention Is All You Need (ê¼¼ê¼¼í•œ ë”¥ëŸ¬ë‹ ë…¼ë¬¸ ë¦¬ë·°ì™€ ì½”ë“œ ì‹¤ìŠµ)](https://www.youtube.com/watch?v=AA621UofTUA)
-->

### ** Free Books: **
<!-- 
Korean:
â˜‘ï¸ [ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/book/2155) - Transformer Fundamentals for NLP (Korean Only)
- (TensorFlow) https://github.com/ukairia777/tensorflow-nlp-tutorial
- (PyTorch) https://wikidocs.net/book/2788
- (Slides) https://www.slideshare.net/wonjoonyoo/ss-188835227
â˜‘ï¸ [PyTorchë¡œ ì‹œì‘í•˜ëŠ” ë”¥ ëŸ¬ë‹ ì…ë¬¸](https://wikidocs.net/book/2788) - See the parts of NLP (Korean Only)
-->

### ** Implementation **
I plan to apply these tools for my implementation:  
âœ… PyTorch, Conda, GPU Server with A5000 x 8, tmux for session management  
<!--
Korean:
â˜‘ï¸ [WandB ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì˜ í•™ìŠµì„ ì¶”ì í•˜ëŠ” ë°©ë²•](https://teddylee777.github.io/machine-learning/wandb/)  
-->
