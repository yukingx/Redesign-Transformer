# Redesign of Efficient Transformers

### **Papers📄 **  
I am reading these papers:  
✅ [Simplifying Transformer Blocks](https://arxiv.org/pdf/2311.01906.pdf)  
☑️ [Blockwise-Parallel-Transformer-for-Long-Context-Large-Models](https://arxiv.org/pdf/2305.19370.pdf)  

### **Goals 🚀**
✅ Understand the concept of Attention, Self-Attention, and Multi-Head Attention.
☑️ 
☑️ Test and Understand a training process of the original transformer at the code level.
☑️ Design and Implement a parallelized transformer block of the encoder
☑️ Design and Implement a simplified transformer block of the encoder

### **Related GitHub Works:**
🌐 [pytorch-llama](https://github.com/hkproj/pytorch-llama/tree/main) - PyTorch implementation of LLaMA by Umar Jamil.  

### **Articles:**
✅ [Beyond Token Prediction: the post-Pretraining journey of modern LLMs](https://amatriain.net/blog/postpretraining)  
