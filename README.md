# Redesign of Efficient Transformers

### ** PapersğŸ“„ **  

Block-level Efficient Transformers:  
âœ… [Simplifying Transformer Blocks](https://arxiv.org/pdf/2311.01906.pdf)  
â˜‘ï¸ [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/pdf/2103.16302.pdf)  
â˜‘ï¸ [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit](https://arxiv.org/pdf/2306.17759.pdf)  
â˜‘ï¸ [Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention](https://arxiv.org/pdf/2112.14000.pdf)  
â˜‘ï¸ [Training-free Transformer Architecture Search](https://arxiv.org/pdf/2203.12217.pdf)    
â˜‘ï¸ [Blockwise-Parallel-Transformer-for-Long-Context-Large-Models](https://arxiv.org/pdf/2305.19370.pdf)  
â˜‘ï¸ [BI-DIRECTIONAL BLOCK SELF-ATTENTION FOR FAST AND MEMORY-EFFICIENT SEQUENCE MODELING](https://arxiv.org/pdf/1804.00857.pdf)  

NAS for Efficient Transformers:  
â˜‘ï¸ [The Evolved Transformer](https://arxiv.org/pdf/1901.11117.pdf)  
â˜‘ï¸ [AutoFormer: Searching Transformers for Visual Recognition](https://arxiv.org/pdf/2107.00651.pdf)  
â˜‘ï¸ [COSFORMER : RETHINKING SOFTMAX IN ATTENTION](https://arxiv.org/pdf/2202.08791.pdf)  
â˜‘ï¸ [NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning](https://arxiv.org/pdf/2306.10792.pdf)  

### ** Goals ğŸš€ **
âœ… Understand the concept of Attention, Self-Attention, and Multi-Head Attention.  
â˜‘ï¸ Understand the concpet of Key, Query, and Value.  
â˜‘ï¸ Understand the concpet of Positional Encoding.  
â˜‘ï¸ Understand the concpet of Encoder and Decoder.  
â˜‘ï¸ Test and Understand a training process of the original transformer at the code level.  
â˜‘ï¸ Design and Implement a parallelized transformer block of the encoder.  
â˜‘ï¸ Design and Implement a simplified transformer block of the encoder.  

### ** Related GitHub Works: **
ğŸŒ [pytorch-llama](https://github.com/hkproj/pytorch-llama/tree/main) - PyTorch implementation of LLaMA by Umar Jamil.  
â˜‘ï¸ Add more repositories. 

### ** Articles: **
â˜‘ï¸ [Beyond Token Prediction: the post-Pretraining journey of modern LLMs](https://amatriain.net/blog/postpretraining)  
â˜‘ï¸ Add more articles. 
