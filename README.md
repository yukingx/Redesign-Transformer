# Redesign of Efficient Transformers

### ** Papers📄 **  

Block-level Efficient Transformers:  
✅ [Simplifying Transformer Blocks](https://arxiv.org/pdf/2311.01906.pdf)  
☑️ [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/pdf/2103.16302.pdf)  
☑️ [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit](https://arxiv.org/pdf/2306.17759.pdf)  
☑️ [Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention](https://arxiv.org/pdf/2112.14000.pdf)  
☑️ [Training-free Transformer Architecture Search](https://arxiv.org/pdf/2203.12217.pdf)    
☑️ [Blockwise-Parallel-Transformer-for-Long-Context-Large-Models](https://arxiv.org/pdf/2305.19370.pdf)  
☑️ [BI-DIRECTIONAL BLOCK SELF-ATTENTION FOR FAST AND MEMORY-EFFICIENT SEQUENCE MODELING](https://arxiv.org/pdf/1804.00857.pdf)  

NAS for Efficient Transformers:  
☑️ [The Evolved Transformer](https://arxiv.org/pdf/1901.11117.pdf)  
☑️ [AutoFormer: Searching Transformers for Visual Recognition](https://arxiv.org/pdf/2107.00651.pdf)  
☑️ [COSFORMER : RETHINKING SOFTMAX IN ATTENTION](https://arxiv.org/pdf/2202.08791.pdf)  
☑️ [NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning](https://arxiv.org/pdf/2306.10792.pdf)  

### ** Goals 🚀 **
✅ Understand the concept of Attention, Self-Attention, and Multi-Head Attention.  
☑️ Understand the concpet of Key, Query, and Value.  
☑️ Understand the concpet of Positional Encoding.  
☑️ Understand the concpet of Encoder and Decoder.  
☑️ Test and Understand a training process of the original transformer at the code level.  
☑️ Design and Implement a parallelized transformer block of the encoder.  
☑️ Design and Implement a simplified transformer block of the encoder.  

### ** Related GitHub Works: **
🌐 [pytorch-llama](https://github.com/hkproj/pytorch-llama/tree/main) - PyTorch implementation of LLaMA by Umar Jamil.  
☑️ Add more repositories. 

### ** Articles: **
☑️ [Beyond Token Prediction: the post-Pretraining journey of modern LLMs](https://amatriain.net/blog/postpretraining)  
☑️ Add more articles. 
